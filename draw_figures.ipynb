{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw FSC/SNIPS\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import add_bow_feat_df\n",
    "from generate_classifier_figures import _decision_tree, _relabel\n",
    "\n",
    "meta = {\n",
    "    \"hubert-base-ls960\": (\"HuBERT-base\", \"-\", \"C0\"),\n",
    "    \"wav2vec2-base\": (\"wav2vec 2.0-base\", \":\", \"C4\"),\n",
    "    \"hubert-large-ll60k\": (\"HuBERT-large\", \".-\", \"C1\"),\n",
    "    \"wav2vec2-large\": (\"wav2vec 2.0-large\", \".:\", \"C3\"),\n",
    "    \"wavlm-large\": (\"WavLM-large\", \"+-\", \"C5\"),\n",
    "    \"wav2vec2-xls-r-300m\": (\"XLS-R-300M\", \"+:\", \"C2\"),\n",
    "}\n",
    "\n",
    "\n",
    "# SNIPS START\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8, 2))\n",
    "\n",
    "df = _relabel(pd.read_pickle(\"tables/snips_close_field.df.pkl\"), Path(\"datasets/mase/slu_splits/snips_close_field/challenge_splits\"))\n",
    "_df = _decision_tree(add_bow_feat_df(df, use_train=True))\n",
    "accs = {\n",
    "    p.stem.split(\"_\")[3]: pd.DataFrame(pickle.load(open(p, \"rb\")))\n",
    "    for p in Path(\"tables\").glob(\"snips*pool-mean*challenge*.pkl\")\n",
    "}\n",
    "\n",
    "for i, task in enumerate((\"speaker_test\", \"utterance_test\"), 1):\n",
    "    for key, (name, marker, color) in meta.items():\n",
    "        ax[i].plot(accs[f\"model-{key}\"][task], marker, label=name, color=color)\n",
    "    ax[i].axhline((_df[_df.split == task].pred == _df[_df.split == task].label).mean(), label=\"Bag of Words\", ls=\"--\", c=\"black\")\n",
    "    title = {\"speaker_test\": \"SPK\", \"utterance_test\": \"UTT\"}[task]\n",
    "    ax[i].set_title(f\"Challenge ({title})\")\n",
    "\n",
    "\n",
    "df = _relabel(pd.read_pickle(\"tables/snips_close_field.df.pkl\"), Path(\"datasets/mase/slu_splits/snips_close_field/original_splits\"))\n",
    "_df = _decision_tree(add_bow_feat_df(df, use_train=True))\n",
    "accs = {\n",
    "    p.stem.split(\"_\")[3]: pd.DataFrame(pickle.load(open(p, \"rb\")))\n",
    "    for p in Path(\"tables\").glob(\"snips*pool-mean*original*.pkl\")\n",
    "}\n",
    "\n",
    "for task in (\"test\", ):\n",
    "    for key, (name, marker, color) in meta.items():\n",
    "        ax[0].plot(accs[f\"model-{key}\"][task], marker, label=name, color=color)\n",
    "    ax[0].axhline((_df[_df.split == task].pred == _df[_df.split == task].label).mean(), label=\"Bag of Words\", ls=\"--\", c=\"black\")\n",
    "    ax[0].set_title(\"Original\")\n",
    "    ax[0].set_xlabel(\"Layer index\")\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[2].legend(ncol=4, loc=\"center\", bbox_to_anchor=(-0.75, 1.35))\n",
    "plt.savefig(\"snips.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# SNIPS END\n",
    "\n",
    "# FSC START\n",
    "fig, ax = plt.subplots(1, 3, figsize=(8, 2))\n",
    "\n",
    "df = _relabel(pd.read_pickle(\"tables/fluent_speech_commands.df.pkl\"), Path(\"datasets/mase/slu_splits/fluent_speech_commands/challenge_splits\"))\n",
    "_df = _decision_tree(add_bow_feat_df(df, use_train=True))\n",
    "accs = {\n",
    "    p.stem.split(\"_\")[3]: pd.DataFrame(pickle.load(open(p, \"rb\")))\n",
    "    for p in Path(\"tables\").glob(\"fluent_*pool-mean*challenge*.pkl\")\n",
    "}\n",
    "\n",
    "for i, task in enumerate((\"speaker_test\", \"utterance_test\"), 1):\n",
    "    for key, (name, marker, color) in meta.items():\n",
    "        ax[i].plot(accs[f\"model-{key}\"][task].dropna(), marker, label=name, color=color)\n",
    "    ax[i].axhline((_df[_df.split == task].pred == _df[_df.split == task].label).mean(), label=\"Bag of Words\", ls=\"--\", c=\"black\")\n",
    "    title = {\"speaker_test\": \"SPK\", \"utterance_test\": \"UTT\"}[task]\n",
    "    ax[i].set_title(f\"Challenge ({title})\")\n",
    "    if task == \"speaker_test\":\n",
    "        ax[i].set_ylim(0.13, 1.025)\n",
    "\n",
    "\n",
    "df = _relabel(pd.read_pickle(\"tables/fluent_speech_commands.df.pkl\"), Path(\"datasets/mase/slu_splits/fluent_speech_commands/original_splits\"))\n",
    "_df = _decision_tree(add_bow_feat_df(df, use_train=True))\n",
    "accs = {\n",
    "    p.stem.split(\"_\")[3]: pd.DataFrame(pickle.load(open(p, \"rb\")))\n",
    "    for p in Path(\"tables\").glob(\"fluent_*pool-mean*original*.pkl\")\n",
    "}\n",
    "\n",
    "for task in (\"test\", ):\n",
    "    for key, (name, marker, color) in meta.items():\n",
    "        ax[0].plot(accs[f\"model-{key}\"][task].dropna(), marker, label=name, color=color)\n",
    "    ax[0].axhline((_df[_df.split == task].pred == _df[_df.split == task].label).mean(), label=\"Bag of Words\", ls=\"--\", c=\"black\")\n",
    "    ax[0].set_title(\"Original\")\n",
    "    ax[0].set_xlabel(\"Layer index\")\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[2].legend(ncol=4, loc=\"center\", bbox_to_anchor=(-0.75, 1.35))\n",
    "plt.savefig(\"fsc.pdf\", bbox_inches=\"tight\")\n",
    "# FSC END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw LibriSpeech/MSW\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "\n",
    "pairs = {\n",
    "    \"MSW\": {\n",
    "        \"left\": \"MSW_model-wavlm-large_slice-True_spk-x_size-2000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"right\": \"MSW_model-wav2vec2-xls-r-300m_slice-True_spk-x_size-2000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"num_seeds\": 5,\n",
    "        \"speakers\": (\"everyone\", ),\n",
    "        \"normalizer\": (\"subtract\", \"subtract\"),\n",
    "        \"legend\": {\"loc\": \"center\", \"bbox_to_anchor\": (-0.2, 1.4), \"ncols\": 2},\n",
    "        \"title\": (\"WavLM-Large\", \"XLS-R-300M\"),\n",
    "    },\n",
    "    \"hubert-spk\": {\n",
    "        \"left\": \"librispeech-dev-clean-test-clean_model-hubert-large-ll60k_slice-True_spk-x_size-full_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"right\": \"librispeech-dev-clean-test-clean_model-hubert-large-ll60k_slice-True_spk-x_size-full_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"num_seeds\": 1,\n",
    "        \"speakers\": (5142, 2412, 6313, 1580, 2277),\n",
    "        \"normalizer\": (\"none\", \"subtract\"),\n",
    "        \"legend\": {\"loc\": \"center\", \"bbox_to_anchor\": (-0.2, 1.4), \"ncols\": 2},\n",
    "        \"title\": (\"HuBERT-large\", \"HuBERT-large (Norm.)\"),\n",
    "    },\n",
    "    \"hubert\": {\n",
    "        \"left\": \"librispeech-dev-clean-test-clean_model-hubert-large-ll60k_slice-True_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"right\": \"librispeech-dev-clean-test-clean_model-hubert-large-ll60k_slice-True_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"num_seeds\": 5,\n",
    "        \"speakers\": (\"everyone\", ),\n",
    "        \"normalizer\": (\"none\", \"subtract\"),\n",
    "        \"legend\": {\"loc\": \"center\", \"bbox_to_anchor\": (-0.2, 1.4), \"ncols\": 3},\n",
    "        \"title\": (\"Audio slicing\", \"Audio slicing (Norm.)\"),\n",
    "    },\n",
    "    \"hubert-slice\": {\n",
    "        \"left\":  \"librispeech-dev-clean-test-clean_model-hubert-large-ll60k_slice-False_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"right\": \"librispeech-dev-clean-test-clean_model-hubert-large-ll60k_slice-False_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"num_seeds\": 5,\n",
    "        \"speakers\": (\"everyone\", ),\n",
    "        \"normalizer\": (\"none\", \"subtract\"),\n",
    "        # \"legend\": (\"left\", {\"loc\": \"lower left\"}),\n",
    "        \"title\": (\"Feature slicing\", \"Feature slicing (Norm.)\"),\n",
    "    },\n",
    "    \"hubert-pool\": {\n",
    "        \"left\": \"librispeech-dev-clean-test-clean_model-hubert-large-ll60k_slice-True_spk-x_size-10000_pool-center_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"right\": \"librispeech-dev-clean-test-clean_model-hubert-large-ll60k_slice-True_spk-x_size-10000_pool-median_cosine_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"num_seeds\": 5,\n",
    "        \"speakers\": (\"everyone\", ),\n",
    "        \"normalizer\": (\"subtract\", \"subtract\"),\n",
    "        \"legend\": {\"loc\": \"center\", \"bbox_to_anchor\": (-0.2, 1.4), \"ncols\": 3},\n",
    "        \"title\": (\"Center pooling (Norm.)\", \"Centroid pooling (Norm.)\"),\n",
    "    },\n",
    "    \"w2v2\": {\n",
    "        \"left\": \"librispeech-dev-clean-test-clean_model-wav2vec2-large_slice-True_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"right\": \"librispeech-dev-clean-test-clean_model-wav2vec2-large_slice-True_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"num_seeds\": 5,\n",
    "        \"speakers\": (\"everyone\", ),\n",
    "        \"normalizer\": (\"none\", \"subtract\"),\n",
    "        # \"legend\": (\"left\", {\"loc\": \"upper left\"}),\n",
    "        \"title\": (\"wav2vec 2.0\", \"wav2vec 2.0 (Norm.)\"),\n",
    "    },\n",
    "    \"base\": {\n",
    "        \"left\": \"librispeech-dev-clean-test-clean_model-hubert-base-ls960_slice-True_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"right\": \"librispeech-dev-clean-test-clean_model-wav2vec2-base_slice-True_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"num_seeds\": 5,\n",
    "        \"speakers\": (\"everyone\", ),\n",
    "        \"normalizer\": (\"subtract\", \"subtract\"),\n",
    "        \"legend\": {\"loc\": \"center\", \"bbox_to_anchor\": (-0.2, 1.4), \"ncols\": 3},\n",
    "        \"title\": (\"HuBERT-base (Norm.)\", \"wav2vec 2.0-base (Norm.)\"),\n",
    "    },\n",
    "    \"large\": {\n",
    "        \"left\": \"librispeech-dev-clean-test-clean_model-hubert-large-ll60k_slice-True_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"right\": \"librispeech-dev-clean-test-clean_model-wav2vec2-large_slice-True_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"num_seeds\": 5,\n",
    "        \"speakers\": (\"everyone\", ),\n",
    "        \"normalizer\": (\"subtract\", \"subtract\"),\n",
    "        \"title\": (\"HuBERT-large (Norm.)\", \"wav2vec 2.0-large (Norm.)\"),\n",
    "    },\n",
    "    \"wavlm-xls\": {\n",
    "        \"left\": \"librispeech-dev-clean-test-clean_model-wavlm-large_slice-True_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"right\": \"librispeech-dev-clean-test-clean_model-wav2vec2-xls-r-300m_slice-True_spk-x_size-10000_pool-mean_seed-x_dist-cos_sim.dist.pkl\",\n",
    "        \"num_seeds\": 5,\n",
    "        \"speakers\": (\"everyone\", ),\n",
    "        \"normalizer\": (\"subtract\", \"subtract\"),\n",
    "        \"title\": (\"WavLM-large (Norm.)\", \"XLS-R-300M (Norm.)\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "keys = {\n",
    "    \"random\": (\"C0\", \"\", \"Random\"),\n",
    "    \"synonym\": (\"C1\", \"x\", \"Synonym\"),\n",
    "    \"homophone\": (\"C2\", \"o\", \"Near homophone\"),\n",
    "    \"speaker\": (\"C3\", \"|\", \"Same speaker\"),\n",
    "    \"same_word\": (\"C4\", \"^\", \"Same word\"),\n",
    "}\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    # Obtained from https://stackoverflow.com/questions/15033511/compute-a-confidence-interval-from-sample-data\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h\n",
    "\n",
    "\n",
    "for pair_key, meta in pairs.items():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    for loc, ax, normalizer, title in zip((\"left\", \"right\"), axes, meta[\"normalizer\"], meta[\"title\"]):\n",
    "        seedwise_dists = []\n",
    "        for seed, speaker in product(range(meta[\"num_seeds\"]), meta[\"speakers\"]):\n",
    "            dists_path = Path(\"/Users/kwangheechoi/Desktop/dev_tables\") / meta[loc].replace(\"spk-x\", f\"spk-{speaker}\").replace(\"seed-x\", f\"seed-{seed}\")\n",
    "            dists = pickle.load(open(dists_path, \"rb\"))\n",
    "            seedwise_dists.append({\n",
    "                k: [mean_confidence_interval(v)[0] for v in vs]\n",
    "                for k, vs in dists.items()\n",
    "            })\n",
    "        agg_dists = {}\n",
    "        for key in seedwise_dists[0].keys():\n",
    "            agg_dists[key] = []\n",
    "            for layer in range(len(seedwise_dists[0][\"random\"])):\n",
    "                vs = [seedwise_dists[i][key][layer] for i in range(len(seedwise_dists))]\n",
    "                agg_dists[key].append(mean_confidence_interval(vs))\n",
    "\n",
    "        for key, tuples in agg_dists.items():\n",
    "            value = np.array([t[0] for t in tuples])\n",
    "            bound = np.array([t[1] for t in tuples])\n",
    "            if normalizer == \"subtract\":\n",
    "                value -= np.array([t[0] for t in agg_dists[\"random\"]])\n",
    "            color, marker, label = keys[key]\n",
    "            style = \"dotted\" if (key == \"random\" and normalizer == \"subtract\") else \"solid\"\n",
    "            ax.plot(value, label=label, marker=marker, color=color, linestyle=style)\n",
    "            ax.fill_between(np.arange(len(value)), value-bound, value+bound, alpha=0.2)\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_ylabel(\"Norm. cos. sim.\" if normalizer == \"subtract\" else \"Cos. sim.\")\n",
    "        ax.set_xlabel(\"Layer index\")\n",
    "        if title in (\"HuBERT-large (Norm.)\", \"Audio slicing (Norm.)\", \"Center pooling (Norm.)\", \"Centroid pooling (Norm.)\"):\n",
    "            ax.set_ylim(-0.02, 0.3)\n",
    "    plt.tight_layout()\n",
    "    if \"legend\" in meta:\n",
    "        plt.legend(**meta[\"legend\"])\n",
    "    plt.savefig(f\"figs/{pair_key}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
